{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Prediction\n",
    "\n",
    "In this demo we estimate the value function for a given policy using Temporal difference method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are at state s_t we take an action based on the current observation,we get an reward and move to the next state S_t+1 and also get a new_observation. Using this we look up the value function and compute the reward for state s_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value_matrix(value_matrix, observation, new_observation, \n",
    "                   reward, alpha, gamma):\n",
    "    \n",
    "    u = value_matrix[observation[0], observation[1]]\n",
    "    u_t1 = value_matrix[new_observation[0], new_observation[1]]\n",
    "    value_matrix[observation[0], observation[1]] += alpha * (reward + gamma * u_t1 - u)\n",
    "    return value_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a gridworld of size (3,4). The terminal states are noted as 1 and others as zero. The agent has four possible actions (UP,DOWN,LEFT,RIGHT) and gets an reward of -0.04 at all states. One of the terminal states has a reward of +1 and other -1 (not favored). Value functions represent how long does it take for an agent to reach the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Matrix:\n",
      "[[ 1.  1.  1. -1.]\n",
      " [ 0. nan  0. -1.]\n",
      " [ 0.  3.  3.  3.]]\n",
      "\n",
      "Value matrix after 1 iterations:\n",
      "[[-0.004 -0.004  0.1    0.   ]\n",
      " [-0.004  0.     0.     0.   ]\n",
      " [-0.004 -0.004 -0.004  0.   ]]\n",
      "\n",
      "Value matrix after 1001 iterations:\n",
      "[[0.874 0.933 0.987 0.   ]\n",
      " [0.826 0.    0.728 0.   ]\n",
      " [0.77  0.708 0.664 0.602]]\n",
      "\n",
      "Value matrix after 2001 iterations:\n",
      "[[0.884 0.943 0.993 0.   ]\n",
      " [0.841 0.    0.847 0.   ]\n",
      " [0.759 0.696 0.661 0.469]]\n",
      "\n",
      "Value matrix after 3001 iterations:\n",
      "[[0.849 0.91  0.959 0.   ]\n",
      " [0.787 0.    0.788 0.   ]\n",
      " [0.73  0.698 0.667 0.54 ]]\n",
      "\n",
      "Value matrix after 4001 iterations:\n",
      "[[0.825 0.874 0.956 0.   ]\n",
      " [0.773 0.    0.527 0.   ]\n",
      " [0.728 0.673 0.624 0.212]]\n",
      "\n",
      "Value matrix after 5001 iterations:\n",
      "[[0.825 0.874 0.955 0.   ]\n",
      " [0.772 0.    0.599 0.   ]\n",
      " [0.703 0.663 0.592 0.323]]\n",
      "\n",
      "Value matrix after 6001 iterations:\n",
      "[[0.831 0.9   0.966 0.   ]\n",
      " [0.767 0.    0.566 0.   ]\n",
      " [0.716 0.678 0.631 0.437]]\n",
      "\n",
      "Value matrix after 7001 iterations:\n",
      "[[0.835 0.929 0.992 0.   ]\n",
      " [0.779 0.    0.666 0.   ]\n",
      " [0.722 0.672 0.636 0.505]]\n",
      "\n",
      "Value matrix after 8001 iterations:\n",
      "[[0.874 0.925 0.949 0.   ]\n",
      " [0.818 0.    0.673 0.   ]\n",
      " [0.758 0.693 0.653 0.331]]\n",
      "\n",
      "Value matrix after 9001 iterations:\n",
      "[[0.893 0.942 0.993 0.   ]\n",
      " [0.83  0.    0.854 0.   ]\n",
      " [0.777 0.716 0.669 0.474]]\n",
      "\n",
      "Value matrix after 10001 iterations:\n",
      "[[0.86  0.917 0.981 0.   ]\n",
      " [0.806 0.    0.683 0.   ]\n",
      " [0.744 0.699 0.653 0.426]]\n",
      "\n",
      "Value matrix after 11001 iterations:\n",
      "[[0.85  0.933 0.99  0.   ]\n",
      " [0.811 0.    0.839 0.   ]\n",
      " [0.735 0.685 0.655 0.544]]\n",
      "\n",
      "Value matrix after 12001 iterations:\n",
      "[[0.842 0.882 0.923 0.   ]\n",
      " [0.802 0.    0.708 0.   ]\n",
      " [0.734 0.688 0.641 0.338]]\n",
      "\n",
      "Value matrix after 13001 iterations:\n",
      "[[0.848 0.917 0.979 0.   ]\n",
      " [0.783 0.    0.387 0.   ]\n",
      " [0.715 0.682 0.629 0.412]]\n",
      "\n",
      "Value matrix after 14001 iterations:\n",
      "[[0.884 0.942 0.966 0.   ]\n",
      " [0.813 0.    0.925 0.   ]\n",
      " [0.752 0.69  0.682 0.543]]\n",
      "\n",
      "Value matrix after 15001 iterations:\n",
      "[[0.852 0.892 0.913 0.   ]\n",
      " [0.79  0.    0.372 0.   ]\n",
      " [0.742 0.691 0.645 0.572]]\n",
      "\n",
      "Value matrix after 16001 iterations:\n",
      "[[0.855 0.913 0.988 0.   ]\n",
      " [0.802 0.    0.651 0.   ]\n",
      " [0.74  0.679 0.614 0.333]]\n",
      "\n",
      "Value matrix after 17001 iterations:\n",
      "[[0.786 0.841 0.847 0.   ]\n",
      " [0.751 0.    0.616 0.   ]\n",
      " [0.709 0.677 0.611 0.412]]\n",
      "\n",
      "Value matrix after 18001 iterations:\n",
      "[[0.823 0.895 0.953 0.   ]\n",
      " [0.757 0.    0.47  0.   ]\n",
      " [0.713 0.67  0.559 0.313]]\n",
      "\n",
      "Value matrix after 19001 iterations:\n",
      "[[0.843 0.894 0.962 0.   ]\n",
      " [0.806 0.    0.785 0.   ]\n",
      " [0.759 0.709 0.658 0.537]]\n",
      "\n",
      "Value matrix after 20001 iterations:\n",
      "[[0.856 0.917 0.983 0.   ]\n",
      " [0.799 0.    0.861 0.   ]\n",
      " [0.744 0.699 0.684 0.42 ]]\n",
      "\n",
      "Value matrix after 21001 iterations:\n",
      "[[0.844 0.908 0.951 0.   ]\n",
      " [0.772 0.    0.411 0.   ]\n",
      " [0.71  0.656 0.614 0.593]]\n",
      "\n",
      "Value matrix after 22001 iterations:\n",
      "[[0.853 0.909 0.959 0.   ]\n",
      " [0.785 0.    0.651 0.   ]\n",
      " [0.701 0.665 0.634 0.282]]\n",
      "\n",
      "Value matrix after 23001 iterations:\n",
      "[[0.847 0.9   0.953 0.   ]\n",
      " [0.797 0.    0.805 0.   ]\n",
      " [0.738 0.689 0.654 0.466]]\n",
      "\n",
      "Value matrix after 24001 iterations:\n",
      "[[0.841 0.893 0.959 0.   ]\n",
      " [0.789 0.    0.629 0.   ]\n",
      " [0.732 0.671 0.596 0.438]]\n",
      "\n",
      "Value matrix after 25001 iterations:\n",
      "[[0.872 0.938 0.994 0.   ]\n",
      " [0.81  0.    0.308 0.   ]\n",
      " [0.752 0.698 0.645 0.423]]\n",
      "\n",
      "Value matrix after 26001 iterations:\n",
      "[[0.86  0.919 0.965 0.   ]\n",
      " [0.803 0.    0.821 0.   ]\n",
      " [0.738 0.692 0.652 0.339]]\n",
      "\n",
      "Value matrix after 27001 iterations:\n",
      "[[0.861 0.91  0.978 0.   ]\n",
      " [0.813 0.    0.733 0.   ]\n",
      " [0.759 0.696 0.663 0.517]]\n",
      "\n",
      "Value matrix after 28001 iterations:\n",
      "[[0.864 0.926 0.952 0.   ]\n",
      " [0.815 0.    0.754 0.   ]\n",
      " [0.771 0.712 0.665 0.393]]\n",
      "\n",
      "Value matrix after 29001 iterations:\n",
      "[[0.854 0.917 0.984 0.   ]\n",
      " [0.79  0.    0.801 0.   ]\n",
      " [0.735 0.676 0.631 0.484]]\n",
      "Value matrix after 30000 iterations:\n",
      "[[0.857 0.914 0.978 0.   ]\n",
      " [0.819 0.    0.751 0.   ]\n",
      " [0.768 0.716 0.658 0.236]]\n"
     ]
    }
   ],
   "source": [
    "#Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])\n",
    "\n",
    "#Define the policy matrix\n",
    "#This is the optimal policy for world with reward=-0.04\n",
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                          [0, np.NaN,  0,  -1],\n",
    "                          [0,      3,  3,   3]])\n",
    "print(\"Policy Matrix:\")\n",
    "print(policy_matrix)\n",
    "\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)\n",
    "\n",
    "value_matrix = np.zeros((3,4))\n",
    "gamma = 0.999\n",
    "alpha = 0.1 #constant step size\n",
    "tot_epoch = 30000\n",
    "print_epoch = 1000\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for step in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        value_matrix = update_value_matrix(value_matrix, observation, \n",
    "                                        new_observation, reward, alpha, gamma)\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done: break\n",
    "\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"Value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix)\n",
    "#Time to check the value matrix obtained\n",
    "print(\"Value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
