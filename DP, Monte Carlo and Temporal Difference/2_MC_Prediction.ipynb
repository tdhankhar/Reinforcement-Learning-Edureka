{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarlo Prediction\n",
    "\n",
    "Given the environment description (policy and transition probabilities ) prediction is the computations done to evaluate the value function when we follow a given policy. This demo intends to evaluate policy using first visit montaecarlo estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gridworld import GridWorld\n",
    "\n",
    "np.set_printoptions(precision=3,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return computation function takes as input a list containing a tuple (state, reward) and the discount factor gamma, the output is a value representing the return from that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return(state_list, gamma):\n",
    "    counter = 0\n",
    "    return_value = 0\n",
    "    for visit in state_list:\n",
    "        reward = visit[1]\n",
    "        return_value += reward * np.power(gamma, counter)\n",
    "        counter += 1\n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Lets create a grid world, the states marked 1 are terminal state and those marked -1 contain obstacles. <br>\n",
    "2.The agent receives a reward of -0.04 for every move from non-terminal states <br>\n",
    "3.The  actions are UP(0), RIGHT(1), DOWN(2) and LEFT(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Matrix:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0. -1.  0.  1.]\n",
      " [ 0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(3, 4)\n",
    "\n",
    "#Define the state matrix\n",
    "state_matrix = np.zeros((3,4))\n",
    "state_matrix[0, 3] = 1\n",
    "state_matrix[1, 3] = 1\n",
    "state_matrix[1, 1] = -1\n",
    "print(\"State Matrix:\")\n",
    "print(state_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also define rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -0.04 -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    " #Define the reward matrix\n",
    "reward_matrix = np.full((3,4), -0.04)\n",
    "reward_matrix[0, 3] = 1\n",
    "reward_matrix[1, 3] = -1\n",
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the transition probabilities for four possible actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Define the transition matrix\n",
    "transition_matrix = np.array([[0.8, 0.1, 0.0, 0.1],\n",
    "                              [0.1, 0.8, 0.1, 0.0],\n",
    "                              [0.0, 0.1, 0.8, 0.1],\n",
    "                              [0.1, 0.0, 0.1, 0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this grid is designed by us, we know the best policy to follow. Lets define it now and evaluate it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1.],\n",
       "       [ 0., nan,  0., -1.],\n",
       "       [ 0.,  3.,  3.,  3.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_matrix = np.array([[1,      1,  1,  -1],\n",
    "                              [0, np.NaN,  0,  -1],\n",
    "                              [0,      3,  3,   3]])\n",
    "policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = GridWorld(3, 4)\n",
    "env.reset()\n",
    "env.setStateMatrix(state_matrix)\n",
    "env.setRewardMatrix(reward_matrix)\n",
    "env.setTransitionMatrix(transition_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -  -  -  * \n",
      " -  #  -  * \n",
      " â—‹  -  -  - \n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "value matrix after 1 iterations:\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "value matrix after 1001 iterations:\n",
      "[[ 0.817  0.875  0.924  1.   ]\n",
      " [ 0.768  0.     0.708 -1.   ]\n",
      " [ 0.716  0.662  0.644  0.518]]\n",
      "\n",
      "value matrix after 2001 iterations:\n",
      "[[ 0.816  0.876  0.927  1.   ]\n",
      " [ 0.769  0.     0.728 -1.   ]\n",
      " [ 0.716  0.665  0.643  0.459]]\n",
      "\n",
      "value matrix after 3001 iterations:\n",
      "[[ 0.813  0.874  0.922  1.   ]\n",
      " [ 0.766  0.     0.699 -1.   ]\n",
      " [ 0.711  0.656  0.625  0.504]]\n",
      "\n",
      "value matrix after 4001 iterations:\n",
      "[[ 0.813  0.873  0.921  1.   ]\n",
      " [ 0.763  0.     0.703 -1.   ]\n",
      " [ 0.71   0.658  0.621  0.503]]\n",
      "\n",
      "value matrix after 5001 iterations:\n",
      "[[ 0.811  0.871  0.921  1.   ]\n",
      " [ 0.761  0.     0.699 -1.   ]\n",
      " [ 0.707  0.655  0.617  0.439]]\n",
      "\n",
      "value matrix after 6001 iterations:\n",
      "[[ 0.812  0.872  0.922  1.   ]\n",
      " [ 0.761  0.     0.7   -1.   ]\n",
      " [ 0.708  0.657  0.617  0.446]]\n",
      "\n",
      "value matrix after 7001 iterations:\n",
      "[[ 0.812  0.872  0.922  1.   ]\n",
      " [ 0.761  0.     0.695 -1.   ]\n",
      " [ 0.709  0.659  0.624  0.448]]\n",
      "\n",
      "value matrix after 8001 iterations:\n",
      "[[ 0.813  0.873  0.922  1.   ]\n",
      " [ 0.762  0.     0.7   -1.   ]\n",
      " [ 0.71   0.659  0.623  0.445]]\n",
      "\n",
      "value matrix after 9001 iterations:\n",
      "[[ 0.813  0.872  0.922  1.   ]\n",
      " [ 0.762  0.     0.698 -1.   ]\n",
      " [ 0.708  0.656  0.623  0.431]]\n",
      "value matrix after 10000 iterations:\n",
      "[[ 0.814  0.873  0.922  1.   ]\n",
      " [ 0.763  0.     0.699 -1.   ]\n",
      " [ 0.709  0.657  0.622  0.397]]\n"
     ]
    }
   ],
   "source": [
    "value_matrix = np.zeros((3,4))\n",
    "#init with 1.0e-10 to avoid division by zero\n",
    "running_mean_matrix = np.full((3,4), 1.0e-10) \n",
    "gamma = 0.999\n",
    "tot_epoch = 10000\n",
    "print_epoch = 1000\n",
    "\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    #Starting a new episode\n",
    "    episode_list = list()\n",
    "    #Reset and return the first observation and reward\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    for _ in range(1000):\n",
    "        #Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        #Move one step in the environment and get obs and reward\n",
    "        observation, reward, done = env.step(action)\n",
    "        #Append the visit in the episode list\n",
    "        episode_list.append((observation, reward))\n",
    "        if done: break\n",
    "    #The episode is finished, now estimating the value function\n",
    "    counter = 0\n",
    "    #Checkup to identify if it is the first visit to a state\n",
    "    checkup_matrix = np.zeros((3,4))\n",
    "    #This cycle is the implementation of First-Visit MC.\n",
    "    #For each state stored in the episode list check if it\n",
    "    #is the first visit then estimate the return.\n",
    "    for visit in episode_list:\n",
    "        observation = visit[0]\n",
    "        row = observation[0]\n",
    "        col = observation[1]\n",
    "        reward = visit[1]\n",
    "        if(checkup_matrix[row, col] == 0):\n",
    "            return_value = get_return(episode_list[counter:], gamma)\n",
    "            running_mean_matrix[row, col] += 1\n",
    "            value_matrix[row, col] += return_value\n",
    "            checkup_matrix[row, col] = 1\n",
    "        counter += 1\n",
    "    if(epoch % print_epoch == 0):\n",
    "        print(\"\")\n",
    "        print(\"value matrix after \" + str(epoch+1) + \" iterations:\") \n",
    "        print(value_matrix / running_mean_matrix)\n",
    "#Time to check the value matrix obtained\n",
    "print(\"value matrix after \" + str(tot_epoch) + \" iterations:\")\n",
    "print(value_matrix / running_mean_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
